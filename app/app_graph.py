import os
from typing import Annotated, List, Optional, Union

from langchain_community.tools import CopyFileTool
from langchain_core.messages import AIMessage, AnyMessage, HumanMessage, ToolMessage
from langchain_core.runnables.config import RunnableConfig
from langchain_core.tools import BaseTool
from langchain_ollama import ChatOllama
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from loguru import logger
from typing_extensions import TypedDict

memory = MemorySaver()

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


class State(TypedDict):
    # Messages have the type "list".
    # The `add_messages` function in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]


class ChatBotNode:
    def __init__(self, tools: List[BaseTool] = []):
        self.llm = ChatOllama(
            base_url=OLLAMA_BASE_URL, model="qwen3:1.7b"
        )  # åˆå§‹åŒ– ChatOllama æ¨¡å‹
        self.llm = self.llm.bind_tools(tools)

    def chatbot(self, state: State):
        return {"messages": [self.llm.invoke(state["messages"])]}


def main():
    graph_builder = StateGraph(State)

    # å·¥å…·
    copy_file_tool = CopyFileTool(root_dir="/tmp/tmprdvsw3tg")
    # å·¥å…·åŒ…: https://python.langchain.ac.cn/docs/integrations/tools/
    tools: List[BaseTool] = [
        copy_file_tool,
    ]

    # åˆå§‹åŒ–èŠ‚ç‚¹
    chatbot = ChatBotNode(tools)

    graph_builder.add_node("chatbot", chatbot.chatbot)
    tool_node = ToolNode(tools=tools)
    graph_builder.add_node("tools", tool_node)

    graph_builder.add_edge(START, "chatbot")
    graph_builder.add_conditional_edges(
        "chatbot",
        tools_condition,
    )
    # å·¥å…·æ‰§è¡Œåè¿”å› chatbot
    graph_builder.add_edge("tools", "chatbot")

    graph = graph_builder.compile(
        checkpointer=memory,
        # å¦‚æœéœ€è¦åœ¨å·¥å…·æ‰§è¡Œå‰ä¸­æ–­ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # interrupt_before=["tools"],
        # interrupt_after=["tools"]
    )

    try:
        graph_img = graph.get_graph().draw_mermaid_png()
        # ä¿å­˜å›¾ç‰‡
        os.makedirs("tmp", exist_ok=True)
        with open("tmp/graph.png", "wb") as f:
            f.write(graph_img)
    except Exception:
        # This requires some extra dependencies and is optional
        pass

    user_inputs = [
        "ä½ æ˜¯è°",
        "æˆ‘æ˜¯é©¬å†¬æ¢…",
        "å¸®æˆ‘è®°ä½æˆ‘çš„è½¦åœåœ¨ C åŒº 3 å·è½¦ä½",
        "æˆ‘æ˜¯è°",
        "æˆ‘åœ¨å“ªé‡Œåœè½¦äº†",
        "è¯·å¸®æˆ‘å¤åˆ¶æ–‡ä»¶åˆ°ç³»ç»Ÿ `/tmp` ç›®å½•",
        "å¸®æˆ‘ä» json ä¸­è·å– key ä¸º `name` çš„å€¼ï¼Œjson å†…å®¹ä¸º `{'name': 'é©¬å†¬æ¢…'}`",
    ]
    user_input_iter = iter(user_inputs)
    while True:
        try:
            user_input = next(user_input_iter)
            # print("ğŸ™‹ User: " + user_input)
            if user_input is None:
                break
            # user_input = input("User: ")
            # if user_input.lower() in ["quit", "exit", "q"]:
            #     print("Goodbye!")
            #     break

            stream_graph_updates(graph, user_input)
        except:
            # # fallback if input() is not available
            # user_input = "What do you know about LangGraph?"
            # print("User: " + user_input)
            # stream_graph_updates(graph, user_input)
            break


def stream_graph_updates(graph: CompiledStateGraph, user_input: str):
    config = RunnableConfig(configurable={"thread_id": "1"})
    print(f"\nğŸ™‹ User: {user_input}")

    try:
        # The config is the **second positional argument** to stream() or invoke()!
        events = graph.stream(
            {"messages": [("user", user_input)]}, config, stream_mode="values"
        )

        for event in events:
            message: AnyMessage = event["messages"][-1]
            # åªè¾“å‡º AI çš„å›å¤å’Œå·¥å…·è°ƒç”¨ç»“æœ
            if isinstance(message, AIMessage):
                print(f"ğŸ¤– Assistant: {message.content}")
                if hasattr(message, "tool_calls") and message.tool_calls:
                    print(f"ğŸ”§ Tool Calls: {message.tool_calls}")
            elif isinstance(message, ToolMessage):
                print(f"ğŸ”¨ Tool Result: {message.content}")
    except Exception as e:
        logger.error(f"Error during graph execution: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
