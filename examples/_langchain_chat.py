from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain.globals import set_debug, set_verbose

set_debug(True)
set_verbose(True)


def demo1():
    prompt = ChatPromptTemplate.from_template("å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
    print("ğŸ“–", type(prompt), prompt)

    model = ChatOllama(model="qwen2.5:3b")  # åˆå§‹åŒ– ChatOllama æ¨¡å‹
    chain = prompt.pipe(model).pipe(StrOutputParser())

    # å•
    response = chain.invoke({"topic": "é¸¡"})
    print(response)

    # æµå¼ä¼ è¾“
    for chunck in chain.stream({"topic": "é¸¡"}):
        print("âœ…", repr(chunck))


def demo2():
    """
    Few-shot
    æä¾›ä¸€äº›å°‘æ ·æœ¬åº§ä½æç¤º
    """

    system = """ä½ æ˜¯ä¸€ä½æ»‘ç¨½çš„å–œå‰§æ¼”å‘˜ã€‚ä½ çš„ä¸“é•¿æ˜¯æ•²é—¨ç¬‘è¯ã€‚ \
    è¿”å›ä¸€ä¸ªåŒ…å«å¼€åœºç™½ï¼ˆå¯¹â€œè°åœ¨é‚£é‡Œï¼Ÿâ€çš„å›ç­”ï¼‰å’Œç»“å°¾ç¬‘ç‚¹ï¼ˆå¯¹â€œ<å¼€åœºç™½>è°ï¼Ÿâ€çš„å›ç­”ï¼‰çš„ç¬‘è¯ã€‚

    ä»¥ä¸‹æ˜¯ä¸€äº›ç¬‘è¯çš„ä¾‹å­ï¼š

    example_user: å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äºé£æœºçš„ç¬‘è¯
    example_assistant: {{"setup": "ä¸ºä»€ä¹ˆé£æœºæ°¸è¿œä¸ä¼šæ„Ÿåˆ°ç–²å€¦ï¼Ÿ", "punchline": "å› ä¸ºä»–ä»¬æœ‰ä¼‘æ¯çš„ç¿…è†€ï¼", "rating": 2}}

    example_user: å‘Šè¯‰æˆ‘å¦ä¸€ä¸ªå…³äºé£æœºçš„ç¬‘è¯
    example_assistant: {{"setup": "è´§ç‰©", "punchline": "è´§ç‰©â€œå—¡å—¡å—¡â€ï¼Œä½†é£æœºâ€œå—¡å—¡å—¡â€ï¼", "rating": 10}}

    example_user: Now about caterpillars
    example_assistant: {{"setup": "æ¯›æ¯›è™«", "punchline": "æ¯›æ¯›è™«çœŸçš„å¾ˆæ…¢ï¼Œä½†çœ‹æˆ‘å˜æˆè´è¶ï¼ŒæŠ¢å°½é£å¤´ï¼", "rating": 5}}"""

    prompt = ChatPromptTemplate.from_messages(
        [("system", system), ("human", "{input}")]
    )

    model = ChatOllama(model="qwen2.5:3b")  # åˆå§‹åŒ– ChatOllama æ¨¡å‹
    chain = prompt.pipe(model).pipe(StrOutputParser())

    # å•
    response = chain.invoke({"input": "é¸¡"})
    print(response)


def demo3():
    examples = [
        HumanMessage("å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äºé£æœºçš„ç¬‘è¯", name="example_user"),
        AIMessage(
            "",
            name="example_assistant",
            tool_calls=[
                {
                    "name": "joke",
                    "args": {
                        "setup": "ä¸ºä»€ä¹ˆé£æœºæ°¸è¿œä¸ä¼šç´¯ï¼Ÿ",
                        "punchline": "å› ä¸ºå®ƒä»¬æœ‰ä¼‘æ¯ç¿…è†€ï¼",
                        "rating": 2,
                    },
                    "id": "1",
                }
            ],
        ),
        # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.
        ToolMessage("", tool_call_id="1"),
        # Some models also expect an AIMessage to follow any ToolMessages,
        # so you may need to add an AIMessage here.
        HumanMessage("Tell me another joke about planes", name="example_user"),
        AIMessage(
            "",
            name="example_assistant",
            tool_calls=[
                {
                    "name": "joke",
                    "args": {
                        "setup": "Cargo",
                        "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!",
                        "rating": 10,
                    },
                    "id": "2",
                }
            ],
        ),
        ToolMessage("", tool_call_id="2"),
        HumanMessage("Now about caterpillars", name="example_user"),
        AIMessage(
            "",
            tool_calls=[
                {
                    "name": "joke",
                    "args": {
                        "setup": "Caterpillar",
                        "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!",
                        "rating": 5,
                    },
                    "id": "3",
                }
            ],
        ),
        ToolMessage("", tool_call_id="3"),
    ]

    system = """ä½ æ˜¯ä¸ªæç¬‘çš„å–œå‰§æ¼”å‘˜ã€‚ä½ çš„ä¸“é•¿æ˜¯æ•²é—¨ç¬‘è¯ã€‚ \
    è¿”å›ä¸€ä¸ªåŒ…å«å¼€åœºç™½ï¼ˆå¯¹â€œè°åœ¨é‚£é‡Œï¼Ÿâ€çš„å›ç­”ï¼‰çš„ç¬‘è¯ \
    å¹¶ä¸”æœ€åçš„ç¬‘ç‚¹ï¼ˆå¯¹â€œ<setup>è°ï¼Ÿâ€çš„å›åº”ï¼‰ã€‚"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("placeholder", "{examples}"),
            ("human", "{input}"),
        ]
    )

    model = ChatOllama(model="qwen2.5:3b")  # åˆå§‹åŒ– ChatOllama æ¨¡å‹
    chain = prompt.pipe(model).pipe(StrOutputParser())

    # å•
    response = chain.invoke({"input": "é¸¡"})
    print(response)


if __name__ == "__main__":
    # demo1()
    # demo2()
    demo3()
