import datetime
import operator
import os
from math import e
from typing import Annotated, List, Tuple, TypedDict, Union

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    BaseMessage,
    ChatMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
from langchain_ollama import ChatOllama
from langgraph.graph import END, START, StateGraph
from langgraph.graph.graph import CompiledGraph
from langgraph.prebuilt import create_react_agent
from loguru import logger
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from modules.tools.baidu_search import BaiduSearchTool

planner_prompt = ChatPromptTemplate.from_messages(
    [
        #         SystemMessage(
        #             """é’ˆå¯¹æ—¢å®šç›®æ ‡ï¼Œåˆ¶å®šä¸€ä¸ªç®€å•çš„åˆ†æ­¥è®¡åˆ’ã€‚
        # æ­¤è®¡åˆ’åº”åŒ…æ‹¬ä¸ªäººä»»åŠ¡ï¼Œå¦‚æœæ­£ç¡®æ‰§è¡Œï¼Œå°†å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„æ­¥éª¤ã€‚
        # æœ€åä¸€æ­¥çš„ç»“æœåº”è¯¥æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿æ¯ä¸€æ­¥éƒ½æœ‰æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯â€”â€”ä¸è¦è·³è¿‡æ­¥éª¤ã€‚"""
        #         ),
        (
            "system",
            """é’ˆå¯¹ç»™å®šçš„ç›®æ ‡ï¼Œåˆ¶å®šä¸€ä¸ªç®€å•çš„åˆ†æ­¥è®¡åˆ’ã€‚

æ­¤è®¡åˆ’åº”åŒ…æ‹¬ä¸ªäººä»»åŠ¡ï¼Œå¦‚æœæ­£ç¡®æ‰§è¡Œï¼Œå°†å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„æ­¥éª¤ã€‚

æœ€åä¸€æ­¥çš„ç»“æœåº”è¯¥æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿æ¯ä¸€æ­¥éƒ½æœ‰æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯â€”â€”ä¸è¦è·³è¿‡æ­¥éª¤ã€‚è¯·ä½¿ç”¨ä¸­æ–‡ã€‚""",
        ),
        ("placeholder", "{messages}"),
    ]
)


replanner_prompt = ChatPromptTemplate.from_template(
    """é’ˆå¯¹ç»™å®šçš„ç›®æ ‡ï¼Œåˆ¶å®šä¸€ä¸ªç®€å•çš„åˆ†æ­¥è®¡åˆ’ã€‚
æ­¤è®¡åˆ’åº”åŒ…æ‹¬ä¸ªäººä»»åŠ¡ï¼Œå¦‚æœæ­£ç¡®æ‰§è¡Œï¼Œå°†å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„æ­¥éª¤ã€‚
æœ€åä¸€æ­¥çš„ç»“æœåº”è¯¥æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿æ¯ä¸€æ­¥éƒ½æœ‰æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯â€”â€”ä¸è¦è·³è¿‡æ­¥éª¤ã€‚

ä½ çš„ç›®æ ‡æ˜¯è¿™ä¸ªï¼š
{input}

ä½ æœ€åˆçš„è®¡åˆ’æ˜¯è¿™æ ·çš„ï¼š
{plan}

æ‚¨ç›®å‰å·²ç»å®Œæˆäº†ä»¥ä¸‹æ­¥éª¤ï¼š
{past_steps}

**è¯´æ˜**:
- å¦‚æœéœ€è¦æ›´å¤šæ­¥éª¤æ‰èƒ½å®ç°ç›®æ ‡ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«å‰©ä½™æ­¥éª¤çš„**è®¡åˆ’**ã€‚
- å¦‚æœæ‰€æœ‰å¿…è¦çš„æ­¥éª¤éƒ½å·²å®Œæˆï¼Œæ ¹æ®æ”¶é›†åˆ°çš„ä¿¡æ¯å‘ç”¨æˆ·è¿”å›ä¸€ä¸ª**å“åº”**ã€‚
- **ä¸è¦**åœ¨æ–°è®¡åˆ’ä¸­åŒ…æ‹¬ä»»ä½•å·²ç»å®Œæˆçš„æ­¥éª¤ã€‚
- Do **not** return an empty plan; if no further steps are needed, you **must** return a **Response**.
- Ensure your output is in the correct structured format as per the `Act` model.

**Remember**:
- The `Act` can be either a `Plan` or a `Response`.
- A `Plan` contains a list of steps that still need to be done.
- A `Response` contains the final answer to the user.

ç›¸åº”åœ°æ›´æ–°æ‚¨çš„è®¡åˆ’ã€‚å¦‚æœæ²¡æœ‰æ›´å¤šæ­¥éª¤éœ€è¦æ‰§è¡Œå¹¶ä¸”æ‚¨å¯ä»¥è¿”å›ç»™ç”¨æˆ·ï¼Œé‚£ä¹ˆå°±é‚£æ ·å›åº”ã€‚å¦åˆ™ï¼Œå¡«å†™è®¡åˆ’ã€‚ã€
åªæ·»åŠ ä»éœ€è¦å®Œæˆçš„æ­¥éª¤åˆ°è®¡åˆ’ä¸­ã€‚ä¸è¦å°†å·²å®Œæˆçš„æ­¥éª¤ä½œä¸ºè®¡åˆ’çš„ä¸€éƒ¨åˆ†è¿”å›ã€‚è¯·ä½¿ç”¨ä¸­æ–‡ã€‚"""
)


class PlanExecute(TypedDict):
    """å®šä¹‰çŠ¶æ€
    ç°åœ¨ï¼Œè®©æˆ‘ä»¬å…ˆå®šä¹‰è¿™ä¸ªä»£ç†çš„è·Ÿè¸ªçŠ¶æ€ã€‚
    é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è·Ÿè¸ªå½“å‰è®¡åˆ’ã€‚è®©æˆ‘ä»¬ç”¨å­—ç¬¦ä¸²åˆ—è¡¨æ¥è¡¨ç¤ºå®ƒã€‚
    æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”è¯¥è·Ÿè¸ªä¹‹å‰æ‰§è¡Œè¿‡çš„æ­¥éª¤ã€‚è®©æˆ‘ä»¬ç”¨å…ƒç»„åˆ—è¡¨æ¥è¡¨ç¤ºï¼ˆè¿™äº›å…ƒç»„å°†åŒ…å«æ­¥éª¤å’Œç»“æœï¼‰ã€‚
    æœ€åï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›çŠ¶æ€æ¥è¡¨ç¤ºæœ€ç»ˆå“åº”ä»¥åŠåŸå§‹è¾“å…¥ã€‚
    """

    input: str
    plan: List[str]
    past_steps: Annotated[List[Tuple], operator.add]
    response: str


class Plan(BaseModel):
    """Plan to follow in future
    è§„åˆ’æ­¥éª¤
    ç°åœ¨è®©æˆ‘ä»¬æ¥æ€è€ƒåˆ›å»ºè§„åˆ’æ­¥éª¤ã€‚è¿™å°†ä½¿ç”¨å‡½æ•°è°ƒç”¨åˆ›å»ºä¸€ä¸ªè®¡åˆ’ã€‚
    ä½¿ç”¨ Pydantic ä¸ LangChain
    """

    steps: List[str] = Field(
        default_factory=[], description="è¦éµå¾ªçš„ä¸åŒæ­¥éª¤ï¼Œåº”è¯¥æŒ‰æ’åºé¡ºåº"
    )


class Response(BaseModel):
    """Response to user."""

    response: str


class Act(BaseModel):
    """Action to perform."""

    # description="Action to perform. If you want to respond to user, use Response. "
    # "If you need to further use tools to get the answer, use Plan."
    action: Union[Response, Plan] = Field(
        description="è¦æ‰§è¡Œçš„è¡ŒåŠ¨. å¦‚æœæ‚¨æƒ³å›å¤ç”¨æˆ·ï¼Œè¯·ä½¿ç”¨å›å¤ã€‚ "
        "å¦‚æœæ‚¨éœ€è¦è¿›ä¸€æ­¥ä½¿ç”¨å·¥å…·æ¥è·å–ç­”æ¡ˆï¼Œè¯·ä½¿ç”¨è®¡åˆ’ã€‚"
    )


class PlannerNode:
    def __init__(self, llm: ChatOllama):
        self.llm = llm
        self.planner = planner_prompt | self.llm.with_structured_output(Plan)

    def run(self, state: PlanExecute):
        logger.info(f"ğŸ§  Planning with state: {state}")
        plan: Plan = self.planner.invoke({"messages": [("user", state["input"])]})
        return {"plan": plan.steps}


class ExecutorNode:
    def __init__(self, graph: CompiledGraph):
        self.graph = graph

    def run(self, state: PlanExecute):
        plan = state["plan"]
        plan_str = "\n".join(f"{i + 1}. {step}" for i, step in enumerate(plan))
        task = plan[0]

        task_formatted = f"""ä»¥ä¸‹è®¡åˆ’ï¼š
    {plan_str}\n\næ‚¨è¢«åˆ†é…æ‰§è¡Œ step {1}, {task}."""
        agent_response = self.graph.invoke({"messages": [("user", task_formatted)]})
        return {
            "past_steps": [(task, agent_response["messages"][-1].content)],
        }


class ReplannerNode:
    def __init__(self, llm: ChatOllama):
        self.llm = llm
        self.replanner = replanner_prompt | self.llm.with_structured_output(Act)

    def run(self, state: PlanExecute):
        logger.info(f"ğŸ§  Replanning with state: {state}")
        output: Act = self.replanner.invoke(state)
        if isinstance(output.action, Response):
            return {"response": output.action.response}
        else:
            return {"plan": output.action.steps}


def should_end(state: PlanExecute):
    if "response" in state and state["response"]:
        return END
    else:
        return "agent"


def main():
    base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
    model_name = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:3b")
    llm = ChatOllama(base_url=base_url, model=model_name, temperature=0)
    # llm.invoke([HumanMessage("ä½ å¥½")])

    tools = [BaiduSearchTool(max_results=10)]
    # tools = [TavilySearchResults(max_results=3)]

    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚
    å¦‚æœä½ ä¸æ¸…æ¥šç­”æ¡ˆï¼Œè¯·ä½¿ç”¨å¯èƒ½çš„å·¥å…·å¸®åŠ©ä½ å®Œæˆä»»åŠ¡
    ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å·¥å…·è¿›è¡ŒæŸ¥çœ‹ç»“æœ:{[tool.name for tool in tools]}
    å½“ä½¿ç”¨å·¥å…·åï¼Œä½ éœ€è¦å¯¹ç»“æœè¿›è¡Œåˆ†æã€‚å¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•ç»§ç»­ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚

    ä»Šå¤©çš„æ—¥æœŸæ˜¯{current_date}ï¼Œç°åœ¨çš„æ—¶é—´æ˜¯{current_time}ï¼Œå›ç­”çš„é—®é¢˜çš„æ—¶å€™éœ€è¦ç»“åˆå½“å‰çš„æ—¶é—´ã€‚
    """
    graph = create_react_agent(llm, tools, prompt=prompt)

    workflow = StateGraph(PlanExecute)

    # Add the plan node
    plan_node = PlannerNode(llm)
    workflow.add_node("planner", plan_node.run)

    # Add the execution step
    def execute_step(state: PlanExecute):
        plan = state["plan"]
        plan_str = "\n".join(f"{i + 1}. {step}" for i, step in enumerate(plan))
        task = plan[0]
        task_formatted = f"""For the following plan:
    {plan_str}\n\nYou are tasked with executing step {1}, {task}."""
        agent_response = graph.invoke({"messages": [("user", task_formatted)]})
        return {
            "past_steps": [(task, agent_response["messages"][-1].content)],
        }

    execute_node = ExecutorNode(graph)
    # workflow.add_node("agent", execute_node.run)
    workflow.add_node("agent", execute_step)

    # Add a replan node
    replan_step = ReplannerNode(llm)
    workflow.add_node("replan", replan_step.run)

    workflow.add_edge(START, "planner")

    # From plan we go to agent
    workflow.add_edge("planner", "agent")

    # From agent, we replan
    workflow.add_edge("agent", "replan")

    workflow.add_conditional_edges(
        "replan",
        # Next, we pass in the function that will determine which node is called next.
        should_end,
        ["agent", END],
    )

    # Finally, we compile it!
    # This compiles it into a LangChain Runnable,
    # meaning you can use it as you would any other runnable
    app = workflow.compile()

    try:
        graph_img = app.get_graph(xray=True).draw_mermaid_png()
        os.makedirs("tmp", exist_ok=True)
        with open("tmp/graph-app.png", "wb") as f:
            f.write(graph_img)
    except Exception:
        # This requires some extra dependencies and is optional
        pass

    # inputs = {"messages": [HumanMessage("å»å¹´ç¾å›½å¤§é€‰çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ")]}
    # for stream in graph.stream(inputs, stream_mode="values"):
    #     message: Union[BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage]
    #     message = stream["messages"][-1]

    #     MESSAGE_ICON = {
    #         SystemMessage: "ğŸ§ª",
    #         HumanMessage: "ğŸ™‹",
    #         AIMessage: "ğŸ¤–",
    #         ToolMessage: "ğŸ› ï¸",
    #     }

    #     logger.info(
    #         f"{MESSAGE_ICON.get(type(message), ' ')} [{message.type}] message: {message}"
    #     )

    config = {"recursion_limit": 50}
    inputs = {"input": "å»å¹´ç¾å›½å¤§é€‰çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ"}
    for event in app.stream(inputs, config=config):
        for k, v in event.items():
            if k != "__end__":
                print(v)


if __name__ == "__main__":
    main()
