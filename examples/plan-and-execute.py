import datetime
import operator
import os
from math import e
from typing import Annotated, List, Tuple, TypedDict, Union

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    BaseMessage,
    ChatMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.prompts.chat import ChatPromptTemplate, MessagesPlaceholder
from langchain_ollama import ChatOllama
from langgraph.graph import END, START, StateGraph
from langgraph.graph.graph import CompiledGraph
from langgraph.prebuilt import create_react_agent
from loguru import logger
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from modules.tools.baidu_search import BaiduSearchTool

planner_prompt = ChatPromptTemplate.from_template(
    """é’ˆå¯¹ç»™å®šçš„ç›®æ ‡ï¼Œåˆ¶å®šä¸€ä¸ªç®€å•çš„åˆ†æ­¥è®¡åˆ’ã€‚
æ­¤è®¡åˆ’åº”åŒ…æ‹¬ä¸ªäººä»»åŠ¡ï¼Œå¦‚æœæ­£ç¡®æ‰§è¡Œï¼Œå°†å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„æ­¥éª¤ã€‚
æœ€åä¸€æ­¥çš„ç»“æœåº”è¯¥æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿æ¯ä¸€æ­¥éƒ½æœ‰æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯â€”â€”ä¸è¦è·³è¿‡æ­¥éª¤ã€‚è¯·ä½¿ç”¨ä¸­æ–‡ã€‚å¹¶ä¸”ä½ éœ€è¦æŒ‰ç…§æŒ‡å®šçš„æ ¼å¼è¾“å‡ºã€‚

ç”¨æˆ·çš„è¾“å…¥æ˜¯è¿™æ ·çš„ï¼š
{messages}
""",
)


planner_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.""",
        ),
        ("placeholder", "{messages}"),
    ]
)

replanner_prompt = ChatPromptTemplate.from_template(
    """é’ˆå¯¹ç»™å®šçš„ç›®æ ‡ï¼Œåˆ¶å®šä¸€ä¸ªç®€å•çš„åˆ†æ­¥è®¡åˆ’ã€‚
æ­¤è®¡åˆ’åº”åŒ…æ‹¬ä¸ªäººä»»åŠ¡ï¼Œå¦‚æœæ­£ç¡®æ‰§è¡Œï¼Œå°†å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„æ­¥éª¤ã€‚
æœ€åä¸€æ­¥çš„ç»“æœåº”è¯¥æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚ç¡®ä¿æ¯ä¸€æ­¥éƒ½æœ‰æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯â€”â€”ä¸è¦è·³è¿‡æ­¥éª¤ã€‚

ä½ çš„ç›®æ ‡æ˜¯è¿™ä¸ªï¼š
{input}

ä½ æœ€åˆçš„è®¡åˆ’æ˜¯è¿™æ ·çš„ï¼š
{plan}

æ‚¨ç›®å‰å·²ç»å®Œæˆäº†ä»¥ä¸‹æ­¥éª¤ï¼š
{past_steps}


æ ¹æ®ä¸Šè¿°çš„ä¿¡æ¯ï¼Œæ‚¨éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œ
- å¦‚æœä½ è®¤ä¸ºä½ å·²ç»æœ‰äº†ç­”æ¡ˆï¼Œè¯·å›å¤ç”¨æˆ·ï¼Œä½¿ç”¨å·¥å…· `Response`ã€‚
- å¦‚æœä½ éœ€è¦è¿›ä¸€æ­¥çš„æ­¥éª¤ï¼Œè¯·å¡«å†™ä¸€ä¸ªæ–°çš„è®¡åˆ’ï¼Œä½¿ç”¨å·¥å…· `Plan`ã€‚åªæ·»åŠ ä»éœ€è¦å®Œæˆçš„æ­¥éª¤åˆ°è®¡åˆ’ä¸­ï¼Œä¸è¦å°†å·²å®Œæˆçš„æ­¥éª¤ä½œä¸ºè®¡åˆ’çš„ä¸€éƒ¨åˆ†è¿”å›ã€‚


**è¯´æ˜**:
- å¦‚æœéœ€è¦æ›´å¤šæ­¥éª¤æ‰èƒ½å®ç°ç›®æ ‡ï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«å‰©ä½™æ­¥éª¤çš„ `Plan`ã€‚
- å¦‚æœæ‰€æœ‰å¿…è¦çš„æ­¥éª¤éƒ½å·²å®Œæˆï¼Œæ ¹æ®æ”¶é›†åˆ°çš„ä¿¡æ¯å‘ç”¨æˆ·è¿”å›ä¸€ä¸ª `Response`ã€‚
- **ä¸è¦**åœ¨æ–°è®¡åˆ’ä¸­åŒ…æ‹¬ä»»ä½•å·²ç»å®Œæˆçš„æ­¥éª¤ã€‚
- **ä¸è¦**è¿”å›ä¸€ä¸ªç©ºè®¡åˆ’ï¼›å¦‚æœæ²¡æœ‰è¿›ä¸€æ­¥çš„æ­¥éª¤éœ€è¦ï¼Œä½ å¿…é¡»è¿”å›ä¸€ä¸ª `Response`ã€‚
- ç¡®ä¿æ‚¨çš„è¾“å‡ºæŒ‰ç…§ `Act` æ¨¡å‹é‡‡ç”¨æ­£ç¡®çš„ç»“æ„åŒ–æ ¼å¼.

**è®°ä½**:
- è¯¥ `Act` å¯ä»¥æ˜¯ `Plan` æˆ– `Response`ã€‚
- ä¸€ä¸ª`Plan`åŒ…å«ä»éœ€å®Œæˆçš„æ­¥éª¤åˆ—è¡¨ã€‚ä»–æ˜¯ä¸€ä¸ªå·¥å…·
- ä¸€ä¸ª `Response` åŒ…å«å¯¹ç”¨æˆ·çš„æœ€ç»ˆç­”æ¡ˆã€‚
"""
)


replanner_prompt = ChatPromptTemplate.from_template(
    """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.

Your objective was this:
{input}

Your original plan was this:
{plan}

You have currently done the follow steps:
{past_steps}

Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan."""
)


class PlanExecute(TypedDict):
    """å®šä¹‰çŠ¶æ€
    ç°åœ¨ï¼Œè®©æˆ‘ä»¬å…ˆå®šä¹‰è¿™ä¸ªä»£ç†çš„è·Ÿè¸ªçŠ¶æ€ã€‚
    é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è·Ÿè¸ªå½“å‰è®¡åˆ’ã€‚è®©æˆ‘ä»¬ç”¨å­—ç¬¦ä¸²åˆ—è¡¨æ¥è¡¨ç¤ºå®ƒã€‚
    æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”è¯¥è·Ÿè¸ªä¹‹å‰æ‰§è¡Œè¿‡çš„æ­¥éª¤ã€‚è®©æˆ‘ä»¬ç”¨å…ƒç»„åˆ—è¡¨æ¥è¡¨ç¤ºï¼ˆè¿™äº›å…ƒç»„å°†åŒ…å«æ­¥éª¤å’Œç»“æœï¼‰ã€‚
    æœ€åï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›çŠ¶æ€æ¥è¡¨ç¤ºæœ€ç»ˆå“åº”ä»¥åŠåŸå§‹è¾“å…¥ã€‚
    """

    input: str
    plan: List[str]
    past_steps: Annotated[List[Tuple], operator.add]
    response: str


class Plan(BaseModel):
    """Plan to follow in future
    è§„åˆ’æ­¥éª¤
    ç°åœ¨è®©æˆ‘ä»¬æ¥æ€è€ƒåˆ›å»ºè§„åˆ’æ­¥éª¤ã€‚è¿™å°†ä½¿ç”¨å‡½æ•°è°ƒç”¨åˆ›å»ºä¸€ä¸ªè®¡åˆ’ã€‚
    ä½¿ç”¨ Pydantic ä¸ LangChain
    """

    steps: List[str] = Field(
        default_factory=[],
        # description="è§„åˆ’æ­¥éª¤ã€‚è¦éµå¾ªçš„ä¸åŒæ­¥éª¤ï¼Œåº”è¯¥æŒ‰æ’åºé¡ºåº",
        description="different steps to follow, should be in sorted order",
    )


class Response(BaseModel):
    """Response to user."""

    response: str


class Act(BaseModel):
    """Action to perform."""

    # description="Action to perform. If you want to respond to user, use Response. "
    # "If you need to further use tools to get the answer, use Plan."
    action: Union[Response, Plan] = Field(
        description="Action to perform. If you want to respond to user, use Response. "
        "If you need to further use tools to get the answer, use Plan."
        #         description=""""è¦æ‰§è¡Œçš„è¡ŒåŠ¨ã€‚
        # - å¦‚æœæ‚¨å·²ç»æœ‰ç¡®å®šçš„ç­”æ¡ˆï¼Œè¯·ä½¿ç”¨å›å¤ (`Response`) å‘Šè¯‰ç”¨æˆ·ç­”æ¡ˆã€‚
        # - å¦‚æœæ‚¨ä¸ç¡®å®šç­”æ¡ˆæ˜¯å¦æ˜¯æ­£ç¡®çš„ï¼Œéœ€è¦è¿›ä¸€æ­¥ä½¿ç”¨å·¥å…·æ¥è·å–ç­”æ¡ˆï¼Œè¯·ä½¿ç”¨è§„åˆ’æ­¥éª¤ (`Plan`)ã€‚"""
    )


class PlannerNode:
    def __init__(self, llm: ChatOllama):
        self.llm = llm
        self.planner_prompt = planner_prompt
        self.planner = planner_prompt | self.llm.with_structured_output(Plan)

    def __call__(self, state: PlanExecute):
        logger.info(f"ğŸ§  Planning with state: {state}")
        inputs = self.planner_prompt.format_prompt(
            messages=[HumanMessage(state["input"])]
        ).to_messages()
        logger.error(f"inputs: {type(inputs)}")
        logger.error(f"inputs: {inputs}")
        structured_llm = self.llm.with_structured_output(Plan, include_raw=True)

        plan: Plan = structured_llm.invoke(inputs)
        logger.error(f"plan: {type(plan)}")
        logger.error(f"plan: {plan}")
        exit()
        plan: Plan = self.planner.invoke({"messages": [HumanMessage(state["input"])]})
        return {"plan": plan.steps}


class ExecutorNode:
    def __init__(self, graph: CompiledGraph):
        self.graph = graph

    def __call__(self, state: PlanExecute):
        logger.info(f"ğŸš— Executor with state: {state}")
        plan = state["plan"]
        plan_str = "\n".join(f"{i + 1}. {step}" for i, step in enumerate(plan))
        task = plan[0]

        task_formatted = f"""ä»¥ä¸‹è®¡åˆ’ï¼š
    {plan_str}\n\næ‚¨è¢«åˆ†é…æ‰§è¡Œ step {1}, {task}."""
        agent_response = self.graph.invoke({"messages": [("user", task_formatted)]})
        return {
            "past_steps": [(task, agent_response["messages"][-1].content)],
        }


class ReplannerNode:
    def __init__(self, llm: ChatOllama):
        self.llm = llm
        self.replanner = replanner_prompt | self.llm.with_structured_output(Act)

    def __call__(self, state: PlanExecute):
        logger.info(f"ğŸ§  Replanning with state: {state}")
        # output: Act = self.replanner.invoke(
        #     {
        #         "input": state["input"],
        #         "plan": state["plan"],
        #         "past_steps": state["past_steps"],
        #     }
        # )
        # logger.error(f"output: {output}")

        prompt = replanner_prompt.format_prompt(
            input=state["input"],
            plan=state["plan"],
            past_steps=state["past_steps"],
        ).to_messages()
        # logger.error(f"[prompt]: {prompt}")

        model_with_structure = self.llm.with_structured_output(Act, include_raw=True)
        output: Act = model_with_structure.invoke(prompt)

        logger.error(f"[output]: {output}")

        exit()

        # if isinstance(output.action, Response):
        #     return {"response": output.action.response}
        # else:
        #     return {"plan": output.action.steps}


def should_end(state: PlanExecute):
    if "response" in state and state["response"]:
        return END
    else:
        return "agent"


def main():
    base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
    model_name = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:3b")
    llm = ChatOllama(base_url=base_url, model=model_name, temperature=0)
    # llm.invoke([HumanMessage("ä½ å¥½")])

    tools = [BaiduSearchTool(max_results=10)]
    # tools = [TavilySearchResults(max_results=3)]

    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚
    å¦‚æœä½ ä¸æ¸…æ¥šç­”æ¡ˆï¼Œè¯·ä½¿ç”¨å¯èƒ½çš„å·¥å…·å¸®åŠ©ä½ å®Œæˆä»»åŠ¡
    ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å·¥å…·è¿›è¡ŒæŸ¥çœ‹ç»“æœ:{[tool.name for tool in tools]}
    å½“ä½¿ç”¨å·¥å…·åï¼Œä½ éœ€è¦å¯¹ç»“æœè¿›è¡Œåˆ†æã€‚å¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•ç»§ç»­ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚

    ä»Šå¤©çš„æ—¥æœŸæ˜¯{current_date}ï¼Œç°åœ¨çš„æ—¶é—´æ˜¯{current_time}ï¼Œå›ç­”çš„é—®é¢˜çš„æ—¶å€™éœ€è¦ç»“åˆå½“å‰çš„æ—¶é—´ã€‚
    """
    graph = create_react_agent(llm, tools, prompt=prompt)

    workflow = StateGraph(PlanExecute)

    # Add the plan node
    plan_node = PlannerNode(llm)
    workflow.add_node("planner", plan_node)

    # Add the execution step
    execute_node = ExecutorNode(graph)
    workflow.add_node("agent", execute_node)

    # Add a replan node
    replan_step = ReplannerNode(llm)
    workflow.add_node("replan", replan_step)

    workflow.add_edge(START, "planner")

    # From plan we go to agent
    workflow.add_edge("planner", "agent")

    # From agent, we replan
    workflow.add_edge("agent", "replan")

    workflow.add_conditional_edges(
        "replan",
        # Next, we pass in the function that will determine which node is called next.
        should_end,
        ["agent", END],
    )

    # Finally, we compile it!
    # This compiles it into a LangChain Runnable,
    # meaning you can use it as you would any other runnable
    app = workflow.compile()

    try:
        graph_img = app.get_graph(xray=8).draw_mermaid_png()
        os.makedirs("tmp", exist_ok=True)
        with open("tmp/graph.png", "wb") as f:
            f.write(graph_img)
    except Exception:
        # This requires some extra dependencies and is optional
        pass

    # inputs = {"messages": [HumanMessage("å»å¹´ç¾å›½å¤§é€‰çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ")]}
    # for stream in graph.stream(inputs, stream_mode="values"):
    #     message: Union[BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage]
    #     message = stream["messages"][-1]

    #     MESSAGE_ICON = {
    #         SystemMessage: "ğŸ§ª",
    #         HumanMessage: "ğŸ™‹",
    #         AIMessage: "ğŸ¤–",
    #         ToolMessage: "ğŸ› ï¸",
    #     }

    #     logger.info(
    #         f"{MESSAGE_ICON.get(type(message), ' ')} [{message.type}] message: {message}"
    #     )

    config = {"recursion_limit": 50}
    inputs = {"input": "å»å¹´ç¾å›½å¤§é€‰çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ"}
    for event in app.stream(inputs, config=config):
        for k, v in event.items():
            if k != "__end__":
                print(v)


if __name__ == "__main__":
    main()
