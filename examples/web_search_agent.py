import asyncio
import os
import re
from typing import Any, List, Tuple, Union

from langchain.agents import (
    AgentExecutor,
    AgentOutputParser,
    BaseSingleActionAgent,
    LLMSingleActionAgent,
    Tool,
)
from langchain.chains.llm import LLMChain
from langchain.prompts import StringPromptTemplate
from langchain.schema import AgentAction, AgentFinish, OutputParserException
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_ollama import ChatOllama
from langgraph.graph.state import CompiledStateGraph
from loguru import logger
from ollama import ResponseError
from pydantic import Field

from modules.prompt import agent_prompt
from modules.tools.baidu_search import BaiduSearchTool


class BaiduSearchAgent(BaseSingleActionAgent):
    """è™šæ‹Ÿè‡ªå®šä¹‰ä»£ç†ã€‚"""

    tool: BaiduSearchTool = Field(
        default_factory=lambda: BaiduSearchTool(max_results=5)
    )

    def __init__(self):
        super().__init__()

    @property
    def input_keys(self):
        return ["query"]

    def plan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[AgentAction, AgentFinish]:
        raise NotImplementedError

    async def aplan(
        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any
    ) -> Union[AgentAction, AgentFinish]:
        """æ ¹æ®è¾“å…¥å†³å®šè¦åšä»€ä¹ˆã€‚

        Args:
            intermediate_steps: LLMåˆ°ç›®å‰ä¸ºæ­¢é‡‡å–çš„æ­¥éª¤ä»¥åŠè§‚å¯Ÿç»“æœ
            **kwargs: ç”¨æˆ·è¾“å…¥

        Returns:
            æŒ‡å®šè¦ä½¿ç”¨çš„å·¥å…·çš„è¡ŒåŠ¨ã€‚
        """
        logger.info(f"ğŸ¤– intermediate_steps: {intermediate_steps}")
        logger.info(f"ğŸ¤– agent å‚æ•°: {kwargs}")

        for action, observation in intermediate_steps:
            logger.info(f"ğŸ¤– [action] {action}  [observation] {observation}")

        return AgentAction(
            tool=self.tool.name,
            tool_input={"query": kwargs["query"], "max_results": 10},
            log="",
        )


async def main():

    tools = [BaiduSearchTool(max_results=5)]

    base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
    # base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model_name = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:3b")
    llm = ChatOllama(base_url=base_url, model=model_name)

    try:
        llm = ChatOllama(base_url=base_url, model=model_name)
        llm.invoke([HumanMessage("ä½ å¥½")])
    except ResponseError as e:
        logger.error(f"ğŸ¤– ChatOllama åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    agent = BaiduSearchAgent()

    agent_executor = AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=tools,
        verbose=True,
    )
    await agent_executor.ainvoke("2023å¹´åŠ æ‹¿å¤§æœ‰å¤šå°‘äººå£ï¼Ÿ")


if __name__ == "__main__":
    asyncio.run(main())
