"""
Build a custom RAG agent with LangGraph
https://docs.langchain.com/oss/python/langgraph/agentic-rag

Split markdown:
https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter
"""

import os
from datetime import datetime
from typing import Literal, Optional

from langchain.tools import tool
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from loguru import logger
from pydantic import BaseModel, Field

print("Loading documents...")
urls = [
    "https://lilianweng.github.io/posts/2024-11-28-reward-hacking/",
    "https://lilianweng.github.io/posts/2024-07-07-hallucination/",
    "https://lilianweng.github.io/posts/2024-04-12-diffusion-video/",
]

docs = [WebBaseLoader(url).load() for url in urls]
# print(repr(docs[0][0].page_content.strip()[:100]))

print("Splitting documents...")
docs_list = [item for sublist in docs for item in sublist]
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=100, chunk_overlap=50
)
doc_splits = text_splitter.split_documents(docs_list)
# print(repr(doc_splits[0].page_content.strip()))


vectorstore = InMemoryVectorStore.from_documents(
    documents=doc_splits, embedding=OllamaEmbeddings(model="qwen3-embedding:0.6b")
)
retriever = vectorstore.as_retriever()


@tool
def retrieve_blog_posts(query: str) -> str:
    """Search and return information about Lilian Weng blog posts."""
    docs = retriever.invoke(query)
    return "\n\n".join([doc.page_content for doc in docs])


retriever_tool = retrieve_blog_posts

retriever_tool.invoke({"query": "types of reward hacking"})


base_url = "http://localhost:11434"
model_name: str = "qwen3:1.7b"
chat_model = ChatOllama(base_url=base_url, model=model_name)


def generate_query_or_respond(state: MessagesState):
    """Call the model to generate a response based on the current state. Given
    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
    """
    response = chat_model.bind_tools([retriever_tool]).invoke(state["messages"])
    return {"messages": [response]}


# input = {"messages": [{"role": "user", "content": "hello!"}]}
input = MessagesState(
    messages=[
        HumanMessage(
            content="What are the different types of reward hacking discussed in Lilian Weng's blog?"
        )
    ]
)
generate_query_or_respond(input)["messages"][-1].pretty_print()


"""
========================
Grade documents
========================
"""

GRADE_PROMPT = (
    "You are a grader assessing relevance of a retrieved document to a user question. \n "
    "Here is the retrieved document: \n\n {context} \n\n"
    "Here is the user question: {question} \n"
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n"
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."
)
GRADE_PROMPT = (
    "æ‚¨æ˜¯ä¸€ä¸ªè¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³æ€§çš„è¯„åˆ†å‘˜ã€‚ \n "
    "ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„æ–‡æ¡£: \n\n {context} \n\n"
    "ä»¥ä¸‹æ˜¯ç”¨æˆ·é—®é¢˜: {question} \n"
    "å¦‚æœæ–‡æ¡£åŒ…å«ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…³é”®è¯æˆ–è¯­ä¹‰æ„ä¹‰ï¼Œåˆ™å°†å…¶è¯„ä¸ºç›¸å…³ï¼Œè¿”å› `yes`ï¼Œå¦‚æœä¸ç›¸å…³è¿”å› `no`ã€‚ \n"
)


class GradeDocuments(BaseModel):
    """Grade documents using a binary score for relevance check."""

    binary_score: str = Field(
        # description="Relevance score: 'yes' if relevant, or 'no' if not relevant"
        description="æè¿°æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ï¼Œ'yes' è¡¨ç¤ºç›¸å…³ï¼Œ'no' è¡¨ç¤ºä¸ç›¸å…³"
    )


base_url = "http://localhost:11434"
model_name: str = "qwen3:1.7b"
grader_model = ChatOllama(base_url=base_url, model=model_name)


def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """Determine whether the retrieved documents are relevant to the question."""
    question = state["messages"][0].content
    context = state["messages"][-1].content

    prompt = GRADE_PROMPT.format(question=question, context=context)
    logger.info(f"ğŸ¤– Grader prompt: {prompt}")
    structured_output_model = grader_model.with_structured_output(
        GradeDocuments, include_raw=True
    )
    response: GradeDocuments = structured_output_model.invoke(
        [HumanMessage(content=prompt)],
    )  # type: ignore

    # å®‰å…¨æå– parsed å¯¹è±¡
    if not isinstance(response, dict):
        logger.error("Unexpected response type from grader model")
        return "rewrite_question"  # æˆ–æŠ›å¼‚å¸¸

    parsed_response: Optional[GradeDocuments] = response.get("parsed")
    parsing_error = response.get("parsing_error")

    if parsing_error:
        logger.error(f"Parsing error: {parsing_error}")
        return "rewrite_question"

    if parsed_response is None:
        logger.warning("Parsed result is None, treating as not relevant")
        return "rewrite_question"

    print(f"Grader response: ({type(response)}) {response}")
    score = parsed_response.binary_score

    if score == "yes":
        return "generate_answer"
    else:
        return "rewrite_question"


"""
========================
Rewrite question
========================
"""

REWRITE_PROMPT = (
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n"
    "Here is the initial question:"
    "\n ------- \n"
    "{question}"
    "\n ------- \n"
    "Formulate an improved question:"
)
REWRITE_PROMPT = (
    "è§‚å¯Ÿè¾“å…¥å¹¶å°è¯•æ¨ç†å…¶èƒŒåçš„è¯­ä¹‰æ„å›¾ `/` å«ä¹‰ã€‚\n"
    "è¿™é‡Œæ˜¯åˆå§‹é—®é¢˜ï¼š"
    "\n ------- \n"
    "{question}"
    "\n ------- \n"
    "æå‡ºä¸€ä¸ªæ”¹è¿›çš„é—®é¢˜:"
)


def rewrite_question(state: MessagesState):
    """Rewrite the original user question."""
    messages = state["messages"]
    question = messages[0].content
    prompt = REWRITE_PROMPT.format(question=question)
    response = chat_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [HumanMessage(content=response.content)]}


"""
========================
Generate an answer
========================
"""
GENERATE_PROMPT = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer the question. "
    "If you don't know the answer, just say that you don't know. "
    "Use three sentences maximum and keep the answer concise.\n"
    "Question: {question} \n"
    "Context: {context}"
)
GENERATE_PROMPT = (
    "æ‚¨æ˜¯é—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ "
    "ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚ "
    "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´æˆ‘ä¸çŸ¥é“ã€‚ "
    "ä½¿ç”¨æœ€å¤šä¸‰å¥è¯ï¼Œå¹¶ä¿æŒå›ç­”ç®€æ´ã€‚\n"
    "é—®é¢˜: {question} \n"
    "ä¸Šä¸‹æ–‡: {context}"
)


def generate_answer(state: MessagesState):
    """Generate an answer."""
    question = state["messages"][0].content
    context = state["messages"][-1].content
    prompt = GENERATE_PROMPT.format(question=question, context=context)
    response = chat_model.invoke([{"role": "user", "content": prompt}])
    return {"messages": [response]}


workflow = StateGraph(MessagesState)

# Define the nodes we will cycle between
workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")

# Decide whether to retrieve
workflow.add_conditional_edges(
    "generate_query_or_respond",
    # Assess LLM decision (call `retriever_tool` tool or respond to the user)
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "retrieve",
        END: END,
    },
)

# Edges taken after the `action` node is called.
workflow.add_conditional_edges(
    "retrieve",
    # Assess agent decision
    grade_documents,
)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

# Compile
graph = workflow.compile()


# graph_mermaid = graph.get_graph().draw_mermaid()
# os.makedirs("tmp", exist_ok=True)
# with open("tmp/test.md", "wb") as f:
#     f.write(f"{datetime.now()}\n```mermaid\n{graph_mermaid}\n```".encode())

for chunk in graph.stream(
    {"messages": [HumanMessage(content="Lilian Wengå¯¹å¥–åŠ±é»‘å®¢çš„ç±»å‹æœ‰ä»€ä¹ˆçœ‹æ³•ï¼Ÿ")]},
    debug=True,
):
    for node, update in chunk.items():
        print("Update from node", node)
        update["messages"][-1].pretty_print()
        print("\n\n")
