import os
import re
from typing import List, Union

from langchain.agents import (
    AgentExecutor,
    AgentOutputParser,
    BaseSingleActionAgent,
    LLMSingleActionAgent,
    Tool,
)
from langchain.chains.llm import LLMChain
from langchain.prompts import BaseChatPromptTemplate, StringPromptTemplate
from langchain.schema import (
    AgentAction,
    AgentFinish,
    HumanMessage,
    OutputParserException,
)
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_ollama import ChatOllama
from langgraph.graph.state import CompiledStateGraph
from loguru import logger
from ollama import ResponseError
from pydantic import Field

from modules.prompt import agent_prompt
from modules.tools.baidu_search import BaiduSearchTool

# è®¾ç½®åŸºæœ¬æ¨¡æ¿
template = """Complete the objective as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

These were previous tasks you completed:



Begin!

Question: {input}
{agent_scratchpad}"""


# è®¾ç½®ä¸€ä¸ªæç¤ºæ¨¡æ¿
class CustomPromptTemplate(BaseChatPromptTemplate):
    # è¦ä½¿ç”¨çš„æ¨¡æ¿
    template: str
    # å¯ç”¨å·¥å…·çš„åˆ—è¡¨
    tools: List[Tool]

    def format_messages(self, **kwargs) -> str:
        # è·å–ä¸­é—´æ­¥éª¤ï¼ˆAgentActionï¼ŒObservationå…ƒç»„ï¼‰
        # ä»¥ç‰¹å®šæ–¹å¼æ ¼å¼åŒ–å®ƒä»¬
        intermediate_steps = kwargs.pop("intermediate_steps")
        thoughts = ""
        for action, observation in intermediate_steps:
            thoughts += action.log
            thoughts += f"\nObservation: {observation}\nThought: "
        # å°†agent_scratchpadå˜é‡è®¾ç½®ä¸ºè¯¥å€¼
        kwargs["agent_scratchpad"] = thoughts
        # ä»æä¾›çš„å·¥å…·åˆ—è¡¨åˆ›å»ºä¸€ä¸ªtoolså˜é‡
        kwargs["tools"] = "\n".join(
            [f"{tool.name}: {tool.description}" for tool in self.tools]
        )
        # ä¸ºæä¾›çš„å·¥å…·åˆ›å»ºä¸€ä¸ªå·¥å…·åç§°åˆ—è¡¨
        kwargs["tool_names"] = ", ".join([tool.name for tool in self.tools])
        formatted = self.template.format(**kwargs)
        return [HumanMessage(content=formatted)]


class CustomOutputParser(AgentOutputParser):

    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:
        # æ£€æŸ¥ä»£ç†æ˜¯å¦åº”è¯¥ç»“æŸ
        if "Final Answer:" in llm_output:
            return AgentFinish(
                # è¿”å›å€¼é€šå¸¸æ˜¯ä¸€ä¸ªå¸¦æœ‰å•ä¸ª`output`é”®çš„å­—å…¸
                # ç›®å‰ä¸å»ºè®®å°è¯•å…¶ä»–ä»»ä½•ä¸œè¥¿ :)
                return_values={"output": llm_output.split("Final Answer:")[-1].strip()},
                log=llm_output,
            )
        # è§£æå‡ºåŠ¨ä½œå’ŒåŠ¨ä½œè¾“å…¥
        regex = r"Action\s*\d*\s*:(.*?)\nAction\s*\d*\s*Input\s*\d*\s*:[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if not match:
            raise ValueError(f"Could not parse LLM output: `{llm_output}`")
        action = match.group(1).strip()
        action_input = match.group(2)
        # è¿”å›åŠ¨ä½œå’ŒåŠ¨ä½œè¾“å…¥
        return AgentAction(
            tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output
        )


def main():
    tools = [BaiduSearchTool(max_results=5)]
    prompt = CustomPromptTemplate(
        template=template,
        tools=tools,
        # è¿™é‡Œçœç•¥äº†`agent_scratchpad`ã€`tools`å’Œ`tool_names`å˜é‡ï¼Œå› ä¸ºè¿™äº›æ˜¯åŠ¨æ€ç”Ÿæˆçš„
        # è¿™é‡ŒåŒ…æ‹¬äº†`intermediate_steps`å˜é‡ï¼Œå› ä¸ºè¿™æ˜¯éœ€è¦çš„
        input_variables=["input", "intermediate_steps"],
    )

    output_parser = CustomOutputParser()

    base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
    # base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model_name = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:3b")

    try:
        llm = ChatOllama(base_url=base_url, model=model_name)
        llm.invoke([HumanMessage("ä½ å¥½")])
    except ResponseError as e:
        logger.error(f"ğŸ¤– ChatOllama åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # LLMé“¾ç”±LLMå’Œæç¤ºç»„æˆ
    llm_chain = LLMChain(llm=llm, prompt=prompt)

    tool_names = [tool.name for tool in tools]
    agent = LLMSingleActionAgent(
        llm_chain=llm_chain,
        output_parser=output_parser,
        stop=["\nObservation:"],
        allowed_tools=tool_names,
    )

    agent_executor = AgentExecutor.from_agent_and_tools(
        agent=agent, tools=tools, verbose=True
    )

    agent_executor.run("Search for Leo DiCaprio's girlfriend on the internet.")


main()
