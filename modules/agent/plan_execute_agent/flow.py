import datetime
import os
import time
from typing import ClassVar, List, cast

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, AnyMessage, HumanMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_ollama import ChatOllama
from langgraph.graph import add_messages
from langgraph.managed import IsLastStep
from loguru import logger
from pydantic import BaseModel, Field
from typing_extensions import Annotated

from modules.agent.react_agent._base import BaseAgent
from modules.tools.baidu_search import BaiduSearchTool

PLANNER_PROMPT = """You are a task planning assistant. Given a task, create a detailed plan.

Task: {input}

Create a plan with the following format:
1. First step
2. Second step
...

Plan:"""

PLANNER_PROMPT_CN = """ä½ æ˜¯ä»»åŠ¡è®¡åˆ’åŠ©æ‰‹ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œåˆ›å»ºä¸€ä¸ªè¯¦ç»†çš„è®¡åˆ’ã€‚

ä»»åŠ¡: {input}

åˆ›å»ºä»¥ä¸‹æ ¼å¼çš„è®¡åˆ’ï¼š
1. ç¬¬ä¸€æ­¥
2. ç¬¬äºŒæ­¥
...

è®¡åˆ’:"""

EXECUTOR_PROMPT = """You are a task executor. Follow the plan and execute each step using available tools:

{tools}

Plan:
{plan}

Current step: {current_step}
Previous results: {previous_results}

Use the following format:
Thought: think about the current step
Action: the action to take
Action Input: the input for the action"""


class AgentState(BaseModel):
    messages: Annotated[List[AnyMessage], add_messages] = Field(default_factory=list)

    is_last_step: IsLastStep = Field(default=False)


class LLM(BaseModel):
    SYSTEM_PROMPT: ClassVar[str] = """You are a helpful AI assistant.
System time: {system_time}"""

    name: str = "call_model"  # Node name

    llm: BaseChatModel = None  # å£°æ˜ llm å­—æ®µ

    max_history_messages: int = Field(
        default=10, description="The maximum number of messages to keep in the history."
    )

    def __init__(self, tools: List = None):
        super().__init__()
        tools = tools or []
        base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
        model_name = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:3b")

        # Initialize the model with tool binding. Change the model or add more tools here.
        self.llm = ChatOllama(base_url=base_url, model=model_name).bind_tools(tools)

    def __call__(self, state: AgentState) -> AgentState:
        """Call the LLM powering our "agent".

        This function prepares the prompt, initializes the model, and processes the response.

        Args:
            state (State): The current state of the conversation.
            config (RunnableConfig): Configuration for the model run.

        Returns:
            dict: A dictionary containing the model's response message.
        """
        # Format the system prompt. Customize this to change the agent's behavior.
        system_message = self.SYSTEM_PROMPT.format(
            system_time=datetime.datetime.now(tz=datetime.timezone.utc).isoformat()
        )

        # Get the model's response
        history_messages = state.messages
        history_messages = history_messages[
            -self.max_history_messages :
        ]  # å†å²æ¶ˆæ¯ï¼ŒTODO: åº”è¯¥è®©æ¨¡å‹å¯¹å†å²å¯¹è¯è¿›è¡Œæ€»ç»“
        response = cast(
            AIMessage,
            self.llm.invoke(
                [
                    # SystemMessage(system_message), # æ˜¯å¦åŠ å…¥ç³»ç»Ÿæ¶ˆæ¯
                    *history_messages,
                ]
            ),
        )

        # Append the model's response to the state's messages
        state.messages.append(response)

        # Handle the case when it's the last step and the model still wants to use a tool
        if state.is_last_step and response.tool_calls:
            return {
                "messages": [
                    AIMessage(
                        id=response.id,
                        content="Sorry, I could not find an answer to your question in the specified number of steps.",
                    )
                ]
            }

        # Return the model's response as a list to be added to existing messages
        return state


class PlanningAgent(BaseAgent):
    active_plan_id: str = Field(default_factory=lambda: f"plan-{int(time.time())}")

    llm: LLM = None  # å£°æ˜ llm å­—æ®µ

    name: str = "planning_agent"  # Node name
    tools: List[BaseTool] = Field(default_factory=list)

    def __init__(self, llm: LLM, tools: List[BaseTool] = None):
        super().__init__()
        self.llm = llm
        if tools is not None:
            self.tools = tools
            self.llm.llm = self.llm.llm.bind_tools(self.tools)
        else:
            self.tools = []

    def __call__(self, state: AgentState) -> AgentState:
        """Create an initial plan based on the request using the flow's LLM and PlanningTool."""
        logger.info(f"Creating initial plan with ID: {self.active_plan_id}")

        # Create a system message for plan creation
        system_message = SystemMessage(
            # "You are a planning assistant. Your task is to create a detailed plan with clear steps."
            # "æ‚¨æ˜¯ä¸€ä½è§„åˆ’åŠ©æ‰‹ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„è®¡åˆ’ï¼Œå¹¶åŒ…å«æ˜ç¡®çš„æ­¥éª¤ã€‚"
            PLANNER_PROMPT_CN.format(input=state.messages[-1].content)
        )

        # Create a user message with the request
        user_message = HumanMessage(
            # f"Create a detailed plan to accomplish this task: {state.messages[-1].content}"
            f"åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„è®¡åˆ’æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡: {state.messages[-1].content}ã€‚"
            f"ä¸ºäº†é¡ºåˆ©å®Œæˆä»»åŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š{[t.name for t in self.tools]}ã€‚"
            f"ä½†æ˜¯åœ¨è§„åˆ’çš„æ—¶å€™è¯·ä¸è¦ä½¿ç”¨å·¥å…·è°ƒç”¨ï¼Œåªéœ€è¦æ˜ç¡®ä½¿ç”¨ä»€ä¹ˆå·¥å…·å³å¯"
        )

        state.messages.append(system_message)
        # state.messages.append(user_message)

        state = self.llm.__call__(state)
        return state


class PlanExecuteAgentFlow:
    def __init__(self):
        pass

    def __call__(self):
        pass


if __name__ == "__main__":
    tools = [BaiduSearchTool(max_results=10)]
    llm_with_tools = LLM(tools)
    state = AgentState(messages=[HumanMessage("ç¾å›½æ€»ç»Ÿæ˜¯è°")])
    # state = llm.__call__(state)
    # print(f"ğŸ£ [{type(state)}: {len(state.messages)}] {state.messages}")
    print()
    for msg in state.messages:
        print(f"ğŸ£ [{msg.type}] {msg}")

    llm = LLM()
    planning_agent = PlanningAgent(llm, tools)
    state = planning_agent.__call__(state)
    # print(f"ğŸ£ [{type(state)}: {len(state.messages)}] {state.messages}")
    print()
    for msg in state.messages:
        print(f"ğŸ£ [{msg.type}] {msg}")
