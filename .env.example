# OLLAMA_MODELS="qwen2.5:3b"
# OLLAMA_BASE_URL="http://ollama-server:11434"


# [
#     CUDA_VISIBLE_DEVICES: 
#     GPU_DEVICE_ORDINAL: 
#     HIP_VISIBLE_DEVICES: 
#     HSA_OVERRIDE_GFX_VERSION: 
#     HTTPS_PROXY: 
#     HTTP_PROXY: 
#     NO_PROXY: 
#     OLLAMA_DEBUG:false 
#     OLLAMA_FLASH_ATTENTION:false 
#     OLLAMA_GPU_OVERHEAD:0 
#     OLLAMA_HOST:http://0.0.0.0:11434 
#     OLLAMA_INTEL_GPU:false 
#     OLLAMA_KEEP_ALIVE:5m0s 
#     OLLAMA_KV_CACHE_TYPE: 
#     OLLAMA_LLM_LIBRARY: 
#     OLLAMA_LOAD_TIMEOUT:5m0s 
#     OLLAMA_MAX_LOADED_MODELS:0 
#     OLLAMA_MAX_QUEUE:512 
#     OLLAMA_MODELS:qwen2.5:3b 
#     OLLAMA_MULTIUSER_CACHE:false 
#     OLLAMA_NOHISTORY:false 
#     OLLAMA_NOPRUNE:false 
#     OLLAMA_NUM_PARALLEL:0 
#     OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] 
#     OLLAMA_SCHED_SPREAD:false 
#     ROCR_VISIBLE_DEVICES: h
#     ttp_proxy: 
#     https_proxy: 
#     no_proxy:
# ]